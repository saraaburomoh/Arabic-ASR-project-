{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kh4AKqasmBQx"
      },
      "source": [
        "# Fine-tuning ArTST for Arabic ASR\n",
        "\n",
        "Dataset: [Classical Arabic TTS Corpus](https://huggingface.co/datasets/MBZUAI/ClArTTS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68woy99umBQ1"
      },
      "outputs": [],
      "source": [
        "! pip install -q transformers datasets librosa evaluate jiwer accelerate transformers[torch] pyarabic sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUrHk5hymBQ2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DR6srRsJmBQ3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\") #prevent printing of warning messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aUIsqQemBQ3"
      },
      "source": [
        "### Data Preprocessing functions\n",
        "\n",
        "Text preprocessing steps described in the paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-10ql5NRmBQ3"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import sys\n",
        "import unicodedata\n",
        "import pyarabic.araby as araby\n",
        "map_numbers = {'0': '٠', '1': '١', '2': '٢', '3': '٣', '4': '٤', '5': '٥', '6': '٦', '7': '٧', '8': '٨', '9': '٩'}\n",
        "map_numbers = dict((v, k) for k, v in map_numbers.items())\n",
        "punctuations = ''.join([chr(i) for i in list(i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P'))])\n",
        "punctuations = punctuations + '÷#ݣ+=|$×⁄<>`åûݘ ڢ̇ پ'\n",
        "\n",
        "def convert_numerals_to_digit(word):\n",
        "    sentence=[]\n",
        "    for w in word:\n",
        "        sentence.append(map_numbers.get(w, w))\n",
        "    word = ''.join(sentence)\n",
        "    return word\n",
        "\n",
        "def remove_diacritics(word):\n",
        "    return araby.strip_diacritics(word)\n",
        "\n",
        "def remove_punctuation(word):\n",
        "    return word.translate(str.maketrans('', '', re.sub('[@% ]','', punctuations))).lower()\n",
        "\n",
        "def normalize_text(text):\n",
        "    # remove diacritics\n",
        "    text = remove_diacritics(text)\n",
        "    # number mapping\n",
        "    text = convert_numerals_to_digit(text)\n",
        "    # punctuation removal\n",
        "    text = remove_punctuation(text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTDOb2q6mBQ4"
      },
      "source": [
        "### Load Dataset from Huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeL-BpWwmBQ5"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"MBZUAI/ClArTTS\")\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZKsCWqGmBQ5"
      },
      "outputs": [],
      "source": [
        "# View dataset features\n",
        "dataset['train'].features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UTfcixMmBQ6"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio\n",
        "\n",
        "# play audio sample\n",
        "print(dataset['train'][0]['text'])\n",
        "Audio(dataset['train'][0]['audio'], rate=dataset['train']['sampling_rate'][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAHHCPiJYkHV"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio\n",
        "\n",
        "# play audio sample\n",
        "print(dataset['train'][5]['text'])\n",
        "Audio(dataset['train'][5]['audio'], rate=dataset['train']['sampling_rate'][5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT8WwAL7mBQ6"
      },
      "source": [
        "### Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iidp5BEkmBQ6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "from transformers import SpeechT5Processor, SpeechT5Tokenizer\n",
        "\n",
        "model_id = \"mbzuai/artst_asr\"\n",
        "tokenizer = SpeechT5Tokenizer.from_pretrained(model_id)\n",
        "processor = SpeechT5Processor.from_pretrained(model_id)\n",
        "sampling_rate = processor.feature_extractor.sampling_rate\n",
        "print(f\"Model expects {sampling_rate} sr\")\n",
        "\n",
        "def prepare_dataset(example):\n",
        "    #  resample audio with librosa\n",
        "    audio = librosa.resample(np.array(example[\"audio\"]), orig_sr=example['sampling_rate'], target_sr=sampling_rate)\n",
        "    text = normalize_text(example[\"text\"]) # text preprocessing steps\n",
        "\n",
        "    # use speecht5 processor for feature extraction, pass in audio, target text\n",
        "    example = processor(\n",
        "        audio=audio,\n",
        "        sampling_rate=sampling_rate,\n",
        "        text_target=text,\n",
        "    )\n",
        "    # # compute input length of audio sample in seconds\n",
        "    example[\"input_length\"] = len(audio) / sampling_rate\n",
        "\n",
        "    return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzO3zp9cmBQ6"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names['train'])\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKukuJLtmBQ7"
      },
      "source": [
        "Finally, we filter any training data with audio samples longer than 30s. We define a function that returns True for samples that are less than 30s, and False for those that are longer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjJ3mAuHmBQ7"
      },
      "outputs": [],
      "source": [
        "max_input_length = 30.0\n",
        "\n",
        "def is_audio_in_length_range(length):\n",
        "    return length < max_input_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9BcYtF_mBQ7"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.filter(\n",
        "    is_audio_in_length_range,\n",
        "    input_columns=[\"input_length\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EHLJBPOmBQ7"
      },
      "source": [
        "### Data Collator for Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85J6uDqYmBQ7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlVDVzzumBQ7"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(\n",
        "        self, features: List[Dict[str, Union[List[int], torch.Tensor]]], padding=True\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "        batch = {}\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        labels_batch = processor.tokenizer.pad({'input_ids':[ sample['labels'] for sample in features]}, return_tensors=\"pt\")\n",
        "\n",
        "        batch['input_values'] = pad_sequence([torch.tensor(sample['input_values'][0]) for sample in features], batch_first=True)\n",
        "        batch['attention_mask'] = pad_sequence([torch.tensor(sample['attention_mask'][0]) for sample in features], batch_first=True)\n",
        "\n",
        "        labels = [{\"labels\": feature[\"labels\"]} for feature in features]\n",
        "\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
        "            labels_batch.attention_mask.ne(1), -100\n",
        "        )\n",
        "\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_H0eq4rOmBQ7"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ddzo8LHImBQ7"
      },
      "source": [
        "### Test Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbNEUT-amBQ7"
      },
      "outputs": [],
      "source": [
        "features = [\n",
        "    dataset['train'][0],\n",
        "    dataset['train'][1],\n",
        "    dataset['train'][2],\n",
        "]\n",
        "\n",
        "batch = data_collator(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hg-RIGSGmBQ7"
      },
      "outputs": [],
      "source": [
        "{k:v.shape for k,v in batch.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2AeJZ4GmBQ8"
      },
      "outputs": [],
      "source": [
        "batch['labels']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9iT4whTmBQ8"
      },
      "outputs": [],
      "source": [
        "batch['input_values'][2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsyF10JmmBQ8"
      },
      "source": [
        "### Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXYPDSOymBQ8"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "wer = evaluate.load(\"wer\")\n",
        "cer = evaluate.load(\"cer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtDhVu1SmBQ8"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # replace -100 with the pad_token_id\n",
        "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    # filtering step to only evaluate the samples that correspond to non-zero references:\n",
        "    pred_str_norm = [\n",
        "        pred_str[i] for i in range(len(pred_str)) if len(label_str[i]) > 0\n",
        "    ]\n",
        "    label_str_norm = [\n",
        "        label_str[i]\n",
        "        for i in range(len(label_str))\n",
        "        if len(label_str[i]) > 0\n",
        "    ]\n",
        "\n",
        "    pred_chr = [w.replace(' ','') for w in pred_str]\n",
        "    label_chr = [w.replace(' ','') for w in label_str]\n",
        "\n",
        "    # compute metrics\n",
        "    _wer = 100 * wer.compute(predictions=pred_str, references=label_str)\n",
        "    _wer_non_zero = 100 * wer.compute(predictions=pred_str_norm, references=label_str_norm)\n",
        "    _cer = 100 * cer.compute(predictions=pred_chr, references=label_chr)\n",
        "\n",
        "    return {\"wer\": _wer, \"cer\": _cer, \"wer_non_zero\": _wer_non_zero}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTUNHI30mBQ8"
      },
      "source": [
        "### Load Pre-trained Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HzS0sqEmBQ8"
      },
      "outputs": [],
      "source": [
        "from transformers import SpeechT5ForSpeechToText\n",
        "\n",
        "model = SpeechT5ForSpeechToText.from_pretrained(model_id)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3vYnyWRmBQ8"
      },
      "outputs": [],
      "source": [
        "# disable cache during training since it's incompatible with gradient checkpointing\n",
        "model.config.use_cache = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsvB4yPbmBQ8"
      },
      "source": [
        "### Define the Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92oZ_HxLmBQ9"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./ASR_Output\",\n",
        "    auto_find_batch_size=True,\n",
        "    per_device_train_batch_size=16,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=6e-5,\n",
        "    lr_scheduler_type=\"inverse_sqrt\",\n",
        "    warmup_steps=100,\n",
        "    max_steps=2000,\n",
        "    gradient_checkpointing=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=8,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=225,\n",
        "    save_steps=250,\n",
        "    eval_steps=250,\n",
        "    logging_steps=100,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        "    report_to=\"tensorboard\",\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bwc4MoxmBQ9"
      },
      "outputs": [],
      "source": [
        "training_data = dataset[\"train\"].train_test_split(test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccTbqf-ImBQ9"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=training_data[\"train\"],\n",
        "    eval_dataset=training_data[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rt8DrVKLmBQ9"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKN0a1FQYXS9"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfNk_YYwmBQ9"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X29qB4v5mBQ9"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate(dataset['test'])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}